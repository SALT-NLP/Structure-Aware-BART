{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(s):\n",
    "    name = ''\n",
    "    sentence = ''\n",
    "    flag = 0\n",
    "    for w in s:\n",
    "        if w != ':' and flag == 0:\n",
    "            name += w\n",
    "        if w == ':':\n",
    "            flag = 1\n",
    "        elif flag == 1:\n",
    "            sentence += w\n",
    "    return name, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversation(data):\n",
    "    conversations = []\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        if len(data[i]['dialogue'].split('\\r\\n')) > 1:\n",
    "            sentences = data[i]['dialogue'].replace(\"|\", \" \").split('\\r\\n')\n",
    "            \n",
    "        else:\n",
    "            sentences = data[i]['dialogue'].replace(\"|\", \" \").split('\\n')\n",
    "            \n",
    "        if len(sentences) == 1:\n",
    "            continue\n",
    "            \n",
    "        conversations.append(sentences)\n",
    "    \n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "import allennlp_models.structured_prediction\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "def coreference(s):\n",
    "    doc = nlp(s)\n",
    "    return doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_coref(full_conv, index, word):\n",
    "    ref_list = ['he', 'she', 'him', 'her', 'it', 'his']\n",
    "    prev_sentence = ''\n",
    "    for i in range(index, -1, -1):\n",
    "        #print(i)\n",
    "        cur_sentence =  full_conv[i][1]\n",
    "        prev_sentence = cur_sentence + ' ' + prev_sentence\n",
    "        coref = coreference(prev_sentence)\n",
    "        #print('----')\n",
    "        #print(prev_sentence)\n",
    "        #print(coref)\n",
    "        #print('----')\n",
    "        if len(coref) > 0:\n",
    "            for clusters in coref:\n",
    "                if word in [str(w) for w in clusters.mentions] and str(clusters.main).lower() not in ref_list:\n",
    "                    return str(clusters.main)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ['i', 'me', 'my']\n",
    "second = ['you', 'u', 'your']\n",
    "third = ['he', 'she', 'him', 'her', 'it', 'his']\n",
    "\n",
    "def transform_perspective(conv):\n",
    "    transformed_conv = ''\n",
    "    temp = []\n",
    "    flag = 0\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(conv)):\n",
    "        #print(i)\n",
    "        cur_name = conv[i][0]\n",
    "        cur_uttr = conv[i][1]\n",
    "        s = ''\n",
    "        \n",
    "\n",
    "        for word in word_tokenize(cur_uttr):\n",
    "            if word.lower() in first:\n",
    "                temp_word = cur_name\n",
    "            elif word.lower() in second:\n",
    "                try:\n",
    "                    temp_word = conv[i+1][0]\n",
    "                except:\n",
    "                    temp_word = conv[i-1][0]\n",
    "            elif word.lower() in third:\n",
    "                temp_word = find_nearest_coref(conv, i, word)\n",
    "            else:\n",
    "                temp_word = word\n",
    "\n",
    "            s = s + temp_word + ' '\n",
    "\n",
    "        #j = len(s)-1\n",
    "        #while s[j] == ' ':\n",
    "        #    j = j - 1\n",
    "\n",
    "        #if s[j] not in punc:\n",
    "        s += '.'\n",
    "\n",
    "\n",
    "        temp.append(s)\n",
    "                \n",
    "                \n",
    "        \n",
    "    transformed_conv = ' '.join(temp)\n",
    "\n",
    "    return transformed_conv, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms+\" \"+starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" +\n",
    "                  alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets +\n",
    "                  \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "\n",
    "    text = re.sub(\"([0-9])\" + \"[.]\" + \"([0-9])\", \"\\\\1<prd>\\\\2\", text)\n",
    "\n",
    "    if \"...\" in text:\n",
    "        text = text.replace(\"...\", \"<prd><prd><prd>\")\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def transformat(name):\n",
    "    with open(name + '.json', encoding = 'utf8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    convs = read_conversation(data)\n",
    "    processed_convs = []\n",
    "    for i in range(0, len(convs)):\n",
    "        temp = []\n",
    "        conv = convs[i]\n",
    "        name_prev = None\n",
    "        same_sentence = ''\n",
    "        for j in range(0, len(conv)):\n",
    "            name, sentence = extract_names(conv[j])\n",
    "\n",
    "            if name != name_prev:\n",
    "                if name_prev is not None:\n",
    "                    temp.append([name_prev, same_sentence])\n",
    "                name_prev = name\n",
    "                same_sentence = sentence\n",
    "            elif name == name_prev:\n",
    "                same_sentence = same_sentence + ' . ' + sentence\n",
    "\n",
    "        temp.append([name_prev, same_sentence])\n",
    "\n",
    "        processed_convs.append(temp)\n",
    "        \n",
    "    transformed_conv = []\n",
    "    for j in tqdm(range(0, len(processed_convs))):\n",
    "        transformed_conv.append(transform_perspective(processed_convs[j])[1])\n",
    "    \n",
    "    return transformed_conv\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [03:52<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_conv = transformat('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_triple(triples):\n",
    "    temp_set = []\n",
    "    for i in range(0, len(triples)):\n",
    "        cur = triples[i]\n",
    "        cur_subject = cur['subject']\n",
    "        cur_relation = cur['relation']\n",
    "        cur_object = cur['object']\n",
    "        \n",
    "        if len(temp_set) == 0:\n",
    "            temp_set.append([cur_subject, cur_relation, cur_object])\n",
    "        else:\n",
    "            flag = 0\n",
    "            #print(temp_set)\n",
    "            for j in range(0, len(temp_set)):\n",
    "                \n",
    "                if temp_set[j][0] == cur_subject and temp_set[j][1] == cur_relation:\n",
    "                    \n",
    "                    if len(cur_object) > len(temp_set[j][2]):\n",
    "                        temp_set[j][2] = cur_object\n",
    "                    flag = 1\n",
    "                    \n",
    "                elif temp_set[j][0] == cur_subject and temp_set[j][2] == cur_object:\n",
    "                    if len(cur_relation) > len(temp_set[j][1]):\n",
    "                        temp_set[j][1] = cur_relation\n",
    "                    flag = 1\n",
    "                    \n",
    "                elif temp_set[j][2] == cur_object and temp_set[j][1] == cur_relation:\n",
    "                    if len(cur_subject) > len(temp_set[j][0]):\n",
    "                        temp_set[j][0] = cur_subject\n",
    "                    flag = 1\n",
    "                    \n",
    "            \n",
    "            if flag == 0:\n",
    "                temp_set.append([cur_subject, cur_relation, cur_object])\n",
    "                    \n",
    "                        \n",
    "    return temp_set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/819 [00:00<01:46,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp /home/jiaaoc/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-825612efbe6e4a48.props -preload openie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [01:25<00:00,  9.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from openie import StanfordOpenIE\n",
    "with StanfordOpenIE() as client:\n",
    "    tuple_set = []\n",
    "    for i in tqdm(range(0, len(transformed_conv))):\n",
    "        triples = []\n",
    "        temp_set = []\n",
    "        for uttr in transformed_conv[i]:\n",
    "            sentences = sentence_tokenize(uttr)\n",
    "            for sent in sentences:\n",
    "                if \"?\" not in sent:\n",
    "                    #print(sent)\n",
    "                    triple =  client.annotate(sent)\n",
    "                    if len(triple) > 0:\n",
    "                        triples.extend(compress_triple(triple))\n",
    "                        \n",
    "        tuple_set.append(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_raw_triples.pkl', 'wb') as f:\n",
    "    pickle.dump(tuple_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_input = []\n",
    "action_adj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "56\n",
      "60\n",
      "62\n",
      "93\n",
      "102\n",
      "103\n",
      "108\n",
      "124\n",
      "140\n",
      "144\n",
      "150\n",
      "158\n",
      "200\n",
      "221\n",
      "230\n",
      "233\n",
      "234\n",
      "251\n",
      "261\n",
      "275\n",
      "293\n",
      "307\n",
      "325\n",
      "338\n",
      "345\n",
      "387\n",
      "393\n",
      "403\n",
      "412\n",
      "441\n",
      "449\n",
      "451\n",
      "486\n",
      "516\n",
      "520\n",
      "547\n",
      "553\n",
      "560\n",
      "579\n",
      "632\n",
      "650\n",
      "657\n",
      "660\n",
      "662\n",
      "665\n",
      "666\n",
      "677\n",
      "684\n",
      "709\n",
      "715\n",
      "725\n",
      "746\n",
      "774\n",
      "781\n",
      "809\n"
     ]
    }
   ],
   "source": [
    "node_num = []\n",
    "\n",
    "for i in range(len(tuple_set)):\n",
    "    node_set = set()\n",
    "    if len(tuple_set[i]) == 0:\n",
    "        print(i)\n",
    "    for u in tuple_set[i]:\n",
    "        node_set.add(u[0])\n",
    "        node_set.add(u[1])\n",
    "        node_set.add(u[2])     \n",
    "    node_num.append(len(node_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(node_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "action_input = []\n",
    "action_adj = []\n",
    "for i in range(0, len(tuple_set)):\n",
    "    id2node = {}\n",
    "    node2id = {}\n",
    "    adj_temp = np.zeros([max(node_num), max(node_num)])\n",
    "    index = 0\n",
    "    if len(tuple_set[i]) == 0:\n",
    "        action_input.append('<pad>')\n",
    "    else:\n",
    "        temp_text = ''\n",
    "        for u in tuple_set[i]:\n",
    "            if u[0] not in node2id:\n",
    "                node2id[u[0]] = index\n",
    "                id2node[index] = u[0]\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[0] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[0] \n",
    "                \n",
    "                index = index + 1\n",
    "            if u[1] not in node2id:\n",
    "                node2id[u[1]] = index\n",
    "                id2node[index] = u[1]\n",
    "                index = index + 1\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[1] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[1] \n",
    "                    \n",
    "            if u[2] not in node2id:\n",
    "                node2id[u[2]] = index\n",
    "                id2node[index] = u[2]\n",
    "                index = index + 1\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[2] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[2] \n",
    "                \n",
    "            adj_temp[node2id[u[0]]][node2id[u[0]]] = 1\n",
    "            adj_temp[node2id[u[1]]][node2id[u[1]]] = 1\n",
    "            adj_temp[node2id[u[2]]][node2id[u[2]]] = 1\n",
    "            \n",
    "            adj_temp[node2id[u[0]]][node2id[u[1]]] = 1\n",
    "            adj_temp[node2id[u[1]]][node2id[u[0]]] = 1\n",
    "            \n",
    "            adj_temp[node2id[u[1]]][node2id[u[2]]] = 1\n",
    "            adj_temp[node2id[u[2]]][node2id[u[1]]] = 1\n",
    "            \n",
    "            \n",
    "    \n",
    "        action_input.append(temp_text)\n",
    "    action_adj.append(adj_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_actions.pkl', 'wb') as f:\n",
    "    pickle.dump(action_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_action_adj.pkl', 'wb') as f:\n",
    "    pickle.dump(action_adj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly you could follow the process to create action graph files for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_actions.pkl', 'wb') as f:\n",
    "    pickle.dump(action_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_action_adj.pkl', 'wb') as f:\n",
    "    pickle.dump(action_adj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
